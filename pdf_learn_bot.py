from dotenv import load_dotenv
import os
import streamlit as st
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.document_loaders import PyPDFLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma

# .env íŒŒì¼ì—ì„œ API ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë„ë¡ ì§€ì •
load_dotenv()

# .env íŒŒì¼ ì•ˆì— <OPENAI_API_KEY = "APIí‚¤ë¡œ ë°”ê¿”ì„œ ê¸°ì…"> ë„£ì–´ì„œ ì €ì¥
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("API key not found in .env file.")

os.environ["OPENAI_API_KEY"] = api_key

st.set_page_config(
    page_title="yo-plan",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# gptê°€ í•™ìŠµí•  ë¬¸ì„œë“¤ ê¸°ì…
pdf_files = [
    r"C:\Users\csh16\Desktop\2024-2\ì¡¸ì—…í”„ë¡œì íŠ¸\dataset\kt_membership_benefits_updated.csv"
]

# Load documents from all PDF files
documents = []
for pdf_file in pdf_files:
    loader = PyPDFLoader(pdf_file)
    documents.extend(loader.load())

# Process documents
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
embeddings = OpenAIEmbeddings()
vector_store = Chroma.from_documents(texts, embeddings)
retriever = vector_store.as_retriever(search_kwargs={"k": 2})

from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

# í•´ë‹¹ ë´‡ì˜ ë§íˆ¬, ì‚¬ìš©ëª©ì , ë‹¹ë¶€ ì‚¬í•­ ê¸°ì…
system_template = """ë„ˆì˜ ì´ë¦„ì€ ìš”ë´‡ì´ì•¼.
            ë„ˆëŠ” í•­ìƒ í•œê¸€ê³¼ ì¡´ëŒ“ë§ì„ í•˜ëŠ” ì±—ë´‡ì´ì•¼.
            ì…ë ¥ë°›ì€ í†µì‹ ì‚¬ì˜ í˜œíƒì„ ì½ê³  ì‚¬ìš©ì ì†Œë¹„ íŒ¨í„´ì— ê°€ì¥ ìµœì í™”ëœ, í˜œíƒì´ ë§ì€ í†µì‹ ì‚¬ë¥¼ ì•Œë ¤ì¤˜.
----------------
{summaries}

You MUST answer in Korean and in Markdown format:"""

messages = [
    SystemMessagePromptTemplate.from_template(system_template),
    HumanMessagePromptTemplate.from_template("{question}")
]

prompt = ChatPromptTemplate.from_messages(messages)

chain_type_kwargs = {"prompt": prompt}

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

chain = RetrievalQAWithSourcesChain.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs=chain_type_kwargs
)

st.subheader('ì§ˆë¬¸ì„ ì ì–´ ì£¼ì„¸ìš”')

def generate_response(input_text):
    result = chain(input_text)
    return result['answer']

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ì§ˆë¬¸ì„ ì ì–´ ì£¼ì„¸ìš” ë¬´ì—‡ì„ ë„ì™€ ë“œë¦´ê¹Œìš”?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    msg = generate_response(prompt)
    st.session_state.messages.append({"role": "assistant", "content": msg})
    st.chat_message("assistant").write(msg)


# ì½”ë“œ ìˆ˜ì •ì´ ëë‚˜ê³  terminalì—ì„œ streamlit run <í•´ë‹¹ íŒŒì¼ ì´ë¦„> ì ì–´ì£¼ë©´ ë©ë‹ˆë‹¹